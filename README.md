# House Prices 
Kaggle-ის კონკურსის მოკლე მიმოხილვა: მოცემული გვაქვს 79 დამახასიათებელი თვისება და ფასი სახლების. ამ ინფორმაციის დახმარებით, ჩვენ უნდა ვივარაუდოთ სხვა სახლების ფასები რაც შეიძლება დიდი სიზუსტით.
ჩემი მიდგომა: დავყავი ეს მონაცემები test და train ნაწილებად. ვალიდაციისათვის გამოვიყენე Kfold, რადგან იყო პატარა dataset. თავიდან ეს ყველა რიგი შევამზადე მოდელებისთვის სათანადოდ. შემდეგ მოვარგე სხვადასხვა მოდელს და ავირჩიე საბოლოოდ ის მოდელი, რომელსაც RMSE ჰქონდა ყველაზე ნაკლები.

# Files
model_experiment- აქ ვმუშაობ train set-ზე, ვაგებ და ვეტსტავ სხვადასხვა მოდელს,ვცდილობ ჰიპერპარამეტრების ოპტიმიზებას GridSearchCV-ითი, ხოლო მოდელებს ვლოგავ mlFlow-ზე.

model_inference- ვა-load-ებ mnFlow-ზე დალოგილ ჩემს საუკეთესო მოდელს და ვაფრედიქდებ ტესტებზე.

# Feature Engineering
1. Nan მნიშვნელობების დამუშავება:
  შევქმენი კლასი NaNHandler Nan მნიშვნელობების შესავსებად
  ცალკე ამოვიღე ცვლადები, რომლებიც იყო არა შემთხვევით გამოტოვებული მნიშვნელობები, არამედ რაღაც მნიშვნელობები (მაგ BsmtCond-ში Nan ნიშნავდა რომ არ ჰქონდა ამ სახლს basement).
  ეს მნიშვნელოვანი ცვლადები სხვანაირად დავჰენდლე, რომ არ "გამეფუჭებინა" ინფორმაცია.
  numerical მახასიათებლები შევავსე მათი საშუალოთი.
  numerical მახასიათებლები შევავსე მათი მოდათი.
  ისეთი სვეტები, სადაც > 50% NaN-ები იყო, არასასარგებლოდ ჩავთვალე და წავშალე.


2.კატეგორიული ცვლადების რიცხვითში გადაყვანა:
  შევქმენი CustomPreprocessor კატეგორიული ცვლადების რიცხვითში გადასაყვანად.
  გამოვიყენე One-Hot Encoding იმ კატეგორიული ცვლადებისთვის, რომელთა კატეგორიების რაოდენობა <= 3.
  გამოვიყენე Ordinal Encoding იმ კატეგორიული ცვლადებისთვის, რომელთა კატეგორიების რაოდენობა > 3.

# Feature Selection
ცვლადების შესარჩევად გამოვიყენე RFE რომ ამერჩია ყველაზე მნიშვნელოვანი ცვლადები.
გავტესტე განსხვავებული ცვლადების რაოდენობა (50, 60, 80) და გამოვიყენე ის, რომელიც საუკეთესო შედეგს დებდა ამ მოდელისთვის.

# Training
მე შევაფასე 5 სხვადასხვა მოდელი:
1. Linear Regression 
აქ გამოვიყენე RFE რათა არასაჭირო ცვლადები მომეცილებინა თავიდან.
გავტესტე სხვადასხვა scaling-ით.
Linear Regression ავიღე საწყის მოდელად, რადგან შედარებით სწრაფია და მარტივია, თუმცა მიდრეკილია Underfitting-ისკენ. შეიძლება ჩემი დატასეტი ბევრად უფრო კომპლექსური იყოს და ამ მოდელმა ვერ დადოს კარგი შედეგი მთლიანობაში.
გამოვიყენე სხვადახვა scaler, რადგან თუ ეს არ ვქენით შეიძლება ზოგი მახასიათებლის დიდმა რიცხვებმა გადაწონოს და არასწორი წარმოდგენა შეგვიქმანს მახასიათებლებზე.
rfe-ში მახასიათებლების რაოდენობა გავტესტე რამენიმე რიცხვზე, რომ შემთხვევით აღებული რიცხვით მნიშვნელოვანი ცვლადებიც არ გამომერიცხა.

მოდელის RMSE: 0.19

2. Ridge Regression 
დავამატე Linear Regression-ს L2 რეგულარიზაცია. ასე overfitting-ის შანსი იქნება ნაკლები.
აქაც გამოვიყენე RFE.
მოდელის RMSE-იც ბევრად გაუმჯობესდა, მაგრამ ეს მოდელიც მაინც linear მოდელია და იგივე პრობლემის წინაშე ვიყავი, რაზეც წინა მოდელზე. ამიტომ გადავწყვიტე სხვაგვარადი მოდელის გამოყენება.
აქ დავამატე ჰიპერპარამეტრად regressor__alpha, რომელიც აკონტროლებს Overfitting არ მოხდეს და კარგად დაი-fit-ოს.

მოდელის RMSE:0.17

3. Random Forest 
დავამატე Random Forest, რადგან ის კარგად მუშაობს არა-ხაზოვან მონაცემებზე.

არ ჩავთვალე RFE საჭიროდ, რადგან ეს მოდელი თავისით ალაგებს ამ ცვალდებს პრიორიტეტულობის მიხედვით.
ასევე ხეები ნაკლებად სენსიტიურები არიან outlier-ების მიმართ ვიდრე linear regression და ridge.
რადგანაც ბევრი ხის საშუალოა პასუხი, overfitting-ის შანსი ნაკლებია.
გამოვიყენე ჰიპერპარამეტრები: regressor__n_estimators, regressor__max_depth, regressor__min_samples_split, regressor__max_features.
regressor__max_depth, min_samples_split - შეიძლება გამოიწვიოს Overfitting, რადგან პატარა leaf-ებისგან შემდგარი ხეები გვექნება და მეტად სენსიტიური იქნება Outlier-ების მიმართ, თუ ამას არ გავუწერთ.

მოდელის RMSE:0.15

4. Gradient Boosting
ეს არის Random Forest-ის უკეთესი ვარიანტი, რადგან განსხვავებით ამისა ის არარანდომულად ქმნის ხეებს. ყოველი შემდეგი ხე სწავლობს წინას შეცდომაზე, რაც ბევრად უკეთესს ხდის პერფორმანსს.
გამოვიყენე ჰიპერპარამეტრები:regressor__n_estimators, regressor__learning_rate, regressor__max_depth, regressor__subsample, regressor__min_samples_split.
learning_rate, max_depth, და min_samples_split აკონტროლებენ overfitting-ს.
learning rate - რაც უფრო მაღალია, მით უფრო სწაფია, მაგრამ რისკია overfit-ის.

მოდელის RMSE:0.1403

5. XGBoost 
აქ დამატებულია Gradient Boosting-ზე რეგულაცია და თანაც თავისით შეუძლია ამ მოდელს ჭკვიანურად პრუნინგი.
გამოვიყენე ჰიპერპარამეტრები: regressor__n_estimators, regressor__learning_rate, regressor__max_depth, regressor__subsample, regressor__colsample_bytree
regressor__colsample_bytree-ეუბნება ცვლადების რა ნაწილი გამოიყენოს შემდეგი ხისთვის. კარგია, რადგან ნაკლებია შანსი Overfit-ის.

მოდელის RMSE:0.1408

# MLflow Tracking
Linear regression- https://dagshub.com/alaki22/alaki22-hw01.mlflow/#/experiments/0/runs/9a17df193c434b3babae1f8497aeb123
Ridge Regression- https://dagshub.com/alaki22/alaki22-hw01.mlflow/#/experiments/0/runs/bce56c2e3c4c4288ac481fd0ad66d9dd
Random Forest- https://dagshub.com/alaki22/alaki22-hw01.mlflow/#/experiments/0/runs/ca6d062d6ead42fca5c33780f1a0a8f5
Gradient Boosting- https://dagshub.com/alaki22/alaki22-hw01.mlflow/#/experiments/0/runs/b1b5e5103e4440d39cd8ccec58e2312b
XGBoost- https://dagshub.com/alaki22/alaki22-hw01.mlflow/#/experiments/0/runs/70cce9166c774e7ca9120fba5cb5246e

Model Metrics: log RMSE
Gradient Boosting და XGBoost  ორივე თითქმის ერთნაირად კარგად მუშაობს, თუმცა  Gradient Boosting ცოტათი ჯობია.
საუკეთესო მოდელის შედეგები:   test_log_rmse = 0.14038382846016093
                               submission result = 0.13656
